{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yet Another DQN Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repo implements a Deep Q-Network as introduced in paper [\"Playing Atari Games with Deep Reinforcement Learning\"](https://arxiv.org/abs/1312.5602). In this notebook I tried to closely follow the ideas and tricks outlined by the paper authors and show how this model can be implemented using the [Pytorch-Lighntning](https://www.pytorchlightning.ai) framework with [Wandb](https://wandb.ai) logging. I highly reccomend reading the paper before reading this notebook, since most of the model logic is explained there.\n",
    "\n",
    "The context of application of such model is peformed within [Gym Car Racing](https://www.gymlibrary.dev/environments/box2d/car_racing/) environment (with discrete actions), which is a simple example of a racing video game, where the user is requred to drive a car within a road as far as possible. \n",
    "\n",
    "The game is finished if the car has visited all tiles or goes outside the road, which in the latter case leads the user to receive a -100 score. Our main task is to score as much as possible.\n",
    "\n",
    "Also do not forget to turn GPU if you wish to run this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (0.15.2)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\raduc\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (2.29.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: pathtools in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (4.22.4)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\raduc\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (1.21.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\raduc\\appdata\\roaming\\python\\python310\\site-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (3.1.31)\n",
      "Requirement already satisfied: setuptools in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from wandb) (66.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from Click!=8.0.0,>=7.0->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\raduc\\repos\\rau_2023\\env\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import BatchSampler, RandomSampler, SequentialSampler\n",
    "from torchvision.transforms import Grayscale\n",
    "\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import wandb\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our implementation of the DQN from the **Replay Memory**, which performs the role of a Dataset and allows the model to sample uncorrelated batches of data. Similar to the original implementation, at each observation we store **n** consecutive frames (in grayscale) of the game, action taken during these **n** frames, received reward, **n** frames of the next episode, and a flag specifying the end of the game. We also specify the size of the replay memory.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(Dataset):\n",
    "    \"\"\"\n",
    "    Class contains all memory needed for a Dataloader to\n",
    "    sample from it, including:\n",
    "    current_observation_memory - observation during which there was a decision to\n",
    "    take an action\n",
    "    action_memory - actions taken at each current observation\n",
    "    reward_memory - reward achieved at each corresponding action\n",
    "    end_state_memory - tensors of booleans indicating whether the game\n",
    "    episode was finished\n",
    "    next_observation_memory - observation which appeared after an action was taken\n",
    "    \n",
    "    Comment on usage of Replay Memory: in argument *observation_shape* we should \n",
    "    input a list of shapes that we expect our observation tensor to have, in particular \n",
    "    in first number we specify the number of channels to store at each observation, \n",
    "    which is the same as the number of frames to perform an action.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 num_samples : int=1000, \n",
    "                 observation_shape : list=[4, 96, 96]):\n",
    "        \n",
    "        self.current_observation_memory = torch.empty(num_samples, *observation_shape)\n",
    "        self.action_memory = torch.empty(num_samples)\n",
    "        self.reward_memory = torch.empty(num_samples)\n",
    "        self.end_state_memory = torch.zeros(num_samples, dtype=torch.bool)\n",
    "        self.next_observation_memory = torch.empty(num_samples, *observation_shape)\n",
    "        self.idx_memory = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get length of the Replay Memory\n",
    "        \"\"\"\n",
    "        return self.current_observation_memory.shape[0]\n",
    "    \n",
    "    def store_observation(self, \n",
    "                          current_observation : torch.Tensor, \n",
    "                          action : int, \n",
    "                          reward : float, \n",
    "                          end_state : bool, \n",
    "                          next_observation : torch.Tensor):\n",
    "        \"\"\"\n",
    "        Stores observation in replay memory\n",
    "        \"\"\"            \n",
    "        index = self.idx_memory % self.current_observation_memory.shape[0]\n",
    "        \n",
    "        self.current_observation_memory[index] = torch.Tensor(current_observation)\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.end_state_memory[index] = end_state\n",
    "        self.next_observation_memory[index] = torch.Tensor(next_observation)\n",
    "        \n",
    "        self.idx_memory += 1\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns an observation from Replay Memory\n",
    "        \"\"\"\n",
    "        index = np.random.randint(low=0, high=self.current_observation_memory.shape[0])\n",
    "        return self.current_observation_memory[index], self.action_memory[index], \\\n",
    "               self.reward_memory[index], self.end_state_memory[index], \\\n",
    "               self.next_observation_memory[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environemnt handles all the interaction between agent, gym environment, and replay memory. It takes as one of its attributes the *Replay Memory* class, and also includes the logic behind *epsilon annealing* strategy, i.e. the gradual decrease in possibility of an agent choosing a random action. We also specify the batch size here which will be used for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(LightningDataModule):\n",
    "    \"\"\"\n",
    "    Environment keeps track of all activity between agent\n",
    "    and gym environment, including management of memory\n",
    "    and preprocessing of observations\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 batch_size : int=8,\n",
    "                 num_samples_memory : int=1000,\n",
    "                 observation_shape : list=[4, 96, 96],\n",
    "                 start_epsilon : float=0.0, \n",
    "                 epsilon_decrease_step : float=0.0, \n",
    "                 stop_epsilon : float=0.0\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.num_samples_memory = num_samples_memory\n",
    "        self.observation_shape = observation_shape\n",
    "        \n",
    "        # ENVIRONMENT\n",
    "        # Create Environment\n",
    "        self.env = gym.make(\"CarRacing-v2\", domain_randomize=False, continuous=False)\n",
    "        \n",
    "        # Set start game flag to True\n",
    "        self.start_game_flag = True\n",
    "        \n",
    "        # PREPROCESSING OF OBSERVATIONS\n",
    "        self.grayscale = Grayscale()\n",
    "        self.frames_per_observation = observation_shape[0]\n",
    "        \n",
    "        # EPSILON\n",
    "        self.epsilon = start_epsilon\n",
    "        self.epsilon_decrease_step = epsilon_decrease_step\n",
    "        self.stop_epsilon = stop_epsilon\n",
    "        \n",
    "        # LOGGING\n",
    "        self.number_positive_games = 0\n",
    "        self.number_negative_games = 0\n",
    "        \n",
    "        # ACTION DICTIONARY\n",
    "        self.action_dict = {0 : 'nothing',\n",
    "                           1 : 'left',\n",
    "                           2 : 'right',\n",
    "                           3 : 'gas',\n",
    "                           4 : 'brake'}\n",
    "        \n",
    "    def setup(self, \n",
    "              stage : bool=None):\n",
    "        \"\"\"\n",
    "        Create a randomly filled replay memory\n",
    "        \"\"\"\n",
    "        # Create Replay Memory\n",
    "        self.replay_memory = ReplayMemory(num_samples=self.num_samples_memory, \n",
    "                                          observation_shape=self.observation_shape)\n",
    "        # Fill Replay Memory\n",
    "        self.fill_replay_memory()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def fill_replay_memory(self):\n",
    "        \"\"\"\n",
    "        Fills replay memory with random actions\n",
    "        \"\"\"\n",
    "        print('...Start filling Replay Memory...')\n",
    "        self.play_game(n_steps=self.num_samples_memory-1,\n",
    "                       model=None,\n",
    "                       random_action=True)\n",
    "        print('...Replay memory is filled...')\n",
    "                \n",
    "    def play_game(self, \n",
    "                  n_steps : int=100, \n",
    "                  model=None,\n",
    "                  random_action : bool=False,\n",
    "                  predict_stage : bool=False,\n",
    "                  play_one_game : bool=False):\n",
    "        \"\"\"\n",
    "        Plays game using a model or taking random action and also\n",
    "        filling the replay memory\n",
    "        Args:\n",
    "        model - DQN model in determining the best action\n",
    "        random_action - bool specifies whether game is played randomly,\n",
    "        used when filling memory\n",
    "        start_epsilon, epsilon_decrease_step, stop_epsilon - arguments\n",
    "        used in defining threshold for choosing a random action\n",
    "        predict_stage - used when predicting the game and testing how\n",
    "        the agent works by producing frames of a game\n",
    "        \"\"\"\n",
    "        # If test_stage, set self.epsilon to 0\n",
    "        # to avoid taking random actions\n",
    "        # and initialise test frame index\n",
    "        if predict_stage:\n",
    "            self.epsilon = 0\n",
    "            self.stop_epsilon = 0\n",
    "            self.test_frame_idx = 0\n",
    "            random_action = False\n",
    "        \n",
    "        for idx in range(n_steps):\n",
    "            # If start game, create a self.current_observation, else\n",
    "            # create a next observation tensor and store it in the memory\n",
    "            if self.start_game_flag:\n",
    "                self.current_observation = self.start_new_game()\n",
    "            else:\n",
    "                # Determine action\n",
    "                action = self.determine_action(model=model,\n",
    "                                               current_observation=self.current_observation,\n",
    "                                               random_action=random_action)\n",
    "                \n",
    "                # Create next_observation, reward, and end_state to store in memory\n",
    "                self.next_observation, reward, end_state = \\\n",
    "                                              self.create_next_observation(action=action,\n",
    "                                                                           predict_stage=predict_stage)\n",
    "                \n",
    "                # Store tensors in memory\n",
    "                self.replay_memory.store_observation(current_observation=self.current_observation, \n",
    "                                                     action=action, \n",
    "                                                     reward=reward, \n",
    "                                                     end_state=end_state, \n",
    "                                                     next_observation=self.next_observation)\n",
    "\n",
    "                \n",
    "                # Assign assign next_observation to current_observatiom\n",
    "                self.current_observation = self.next_observation\n",
    "                \n",
    "                # Decrease epsilon if possible and if not random action\n",
    "                if not random_action:\n",
    "                    self.decrement_epsilon()\n",
    "                    \n",
    "                if play_one_game and end_state:\n",
    "                    break\n",
    "            \n",
    "            # Log necessary values if not random_action\n",
    "            if not random_action and not predict_stage:\n",
    "                self.log_wandb()\n",
    "            \n",
    "    def log_wandb(self):\n",
    "        \"\"\"\n",
    "        Logs all wanted values into wandb\n",
    "        \"\"\"\n",
    "        wandb.log({'epsilon' : self.epsilon})\n",
    "    \n",
    "    def determine_action(self,\n",
    "                         current_observation : int,\n",
    "                         random_action : bool,\n",
    "                         model=None,\n",
    "                         ):\n",
    "        \"\"\"\n",
    "        Determine an action of current_observation\n",
    "        randomly or based on model\n",
    "        \"\"\"\n",
    "        # Determine action\n",
    "        if random_action or (np.random.uniform() < self.epsilon):\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = model(current_observation).max(dim=1).indices.item()\n",
    "        return action\n",
    "    \n",
    "    def decrement_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decrements epsilon by self.epsilon_decrease_step\n",
    "        if it is larger than self.stop_epsilon\n",
    "        \n",
    "        Note that the decrement of epsilon is adjusted according\n",
    "        to the number of frames passed\n",
    "        \"\"\"\n",
    "        if self.epsilon > self.stop_epsilon:\n",
    "            decrement_epsilon = self.frames_per_observation * self.epsilon_decrease_step\n",
    "            self.epsilon = max(self.stop_epsilon, self.epsilon - decrement_epsilon)\n",
    "        \n",
    "    \n",
    "    def create_next_observation(self,\n",
    "                                action : int,\n",
    "                                predict_stage : bool=False):\n",
    "        \"\"\"\n",
    "        Create a next_observation tensor by running\n",
    "        through self.frames_per_observation frames a \n",
    "        prespecified action\n",
    "        \"\"\"\n",
    "        # Define list of frames for a chosen action\n",
    "        # and a reward\n",
    "        frames_action = []\n",
    "        reward_action = 0\n",
    "        \n",
    "\n",
    "            \n",
    "        for idx_frame in range(self.frames_per_observation):\n",
    "            frame, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            \n",
    "            # If test_stage, save image to disk\n",
    "            if predict_stage:\n",
    "                if not 'test_game' in os.listdir():\n",
    "                    subprocess.run('mkdir test_game'.split())\n",
    "                    \n",
    "                im = Image.fromarray(frame)\n",
    "                im.save(f\"test_game/frame_{self.test_frame_idx}_action_{self.action_dict[action]}.jpeg\")\n",
    "                self.test_frame_idx += 1\n",
    "                \n",
    "            # Update the reward of action\n",
    "            reward_action += reward\n",
    "            # Change value of the game\n",
    "            self.game_total_return += reward\n",
    "            # Get end state\n",
    "            end_state = terminated or truncated\n",
    "\n",
    "            # if end_state, call process_end_game and exit for loop\n",
    "            if end_state:\n",
    "                if not predict_stage:\n",
    "                    frames_action = self.process_end_game(frames_action=frames_action,\n",
    "                                                      last_frame=frame)\n",
    "                break\n",
    "\n",
    "            frames_action.append(frame)\n",
    "\n",
    "        # Create a post-processed observation tensor\n",
    "        next_observation = self.preprocess_frames(*frames_action)\n",
    "        return next_observation, reward_action, end_state\n",
    "        \n",
    "    \n",
    "    def preprocess_frames(self,\n",
    "                          *frames : np.array):\n",
    "        \"\"\"\n",
    "        Preprocesses all frames received from env\n",
    "        and returns a tensor ready to be put in memory\n",
    "        \"\"\"\n",
    "        out_memory = []\n",
    "        # Apply grayscale to each of the observations\n",
    "        for frame_ in frames:\n",
    "            frame = torch.Tensor(frame_).transpose(0, 2)\n",
    "            frame = self.grayscale(frame)\n",
    "            out_memory.append(frame)\n",
    "            \n",
    "        # Concacenate tensors\n",
    "        out = torch.cat(out_memory)\n",
    "        return out\n",
    "        \n",
    "    def process_end_game(self, \n",
    "                         frames_action : list,\n",
    "                         last_frame : np.array):\n",
    "        \"\"\"\n",
    "        Procesess the game if it has reached\n",
    "        an end state by filling the remaining\n",
    "        frames_action with the last frame\n",
    "        \"\"\"\n",
    "        frames_action = frames_action.copy()\n",
    "        while len(frames_action) < self.frames_per_observation:\n",
    "            frames_action.append(last_frame)\n",
    "            \n",
    "        # Set flag to start a new game to true\n",
    "        self.start_game_flag = True\n",
    "        \n",
    "        # Log game_total_return\n",
    "        wandb.log({\"Game Total Return\" : self.game_total_return})\n",
    "        \n",
    "        # Log either plus 1 positive or negative game\n",
    "        if self.game_total_return >= 0:\n",
    "            self.number_positive_games += 1\n",
    "        else:\n",
    "            self.number_negative_games += 1\n",
    "        wandb.log({\"number_positive_games\" : self.number_positive_games,\n",
    "                   \"number_negative_games\" : self.number_negative_games})\n",
    "        \n",
    "        return frames_action\n",
    "        \n",
    "    \n",
    "    def start_new_game(self):\n",
    "        \"\"\"\n",
    "        Starts new game by observing\n",
    "        first self.frames_per_observation and\n",
    "        taking action of not moving for first\n",
    "        self.frames_per_observations\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reset environment\n",
    "        start_frame, _ = self.env.reset()\n",
    "        # Assign first frame to tensor\n",
    "        frames_start = [start_frame,]\n",
    "        \n",
    "        # Assign other self.frames_per_observation-1 \n",
    "        # frames to frames_start\n",
    "        for frame_ in range(self.frames_per_observation-1):\n",
    "            frame, a_, b_, c_, d_ = self.env.step(0)\n",
    "            frames_start.append(frame)\n",
    "        \n",
    "        # Set self.start_game_flag to false\n",
    "        self.start_game_flag = False\n",
    "        \n",
    "        # Set value of the game to zero\n",
    "        self.game_total_return = 0\n",
    "        \n",
    "        # Return processed start frames\n",
    "        return self.preprocess_frames(*frames_start)      \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Dataloader used for training\n",
    "        \n",
    "        To see what Dataloaders output, use code below:\n",
    "        ---------------------\n",
    "        env = Environment()\n",
    "        env.setup(None)\n",
    "        it = iter(env.train_dataloader())\n",
    "        current_observations, actions, rewards, \\\n",
    "        end_states, next_observations = next(it)\n",
    "        ---------------------\n",
    "        \"\"\"\n",
    "        return DataLoader(self.replay_memory, batch_size=self.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Network\n",
    "\n",
    "This is a standard Pytorch implementation of the convolutional neural network, which is almost identical to the neural network outlined in the paper, i.e. it takes two convolutional layers, followed by two linear layers with ReLU units between all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    DQN netowrk with two convolutional\n",
    "    and two linear layers with ReLU \n",
    "    nonlinearity\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape : list=[4,96,96],\n",
    "                 out_channels1 : int=5,\n",
    "                 out_channels2 : int=5,\n",
    "                 kernel_size : int=4,\n",
    "                 stride : int=2,\n",
    "                 hidden_size : int=256,\n",
    "                 num_actions : int=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_shape[0], \n",
    "                               out_channels=out_channels1, \n",
    "                               kernel_size=kernel_size, \n",
    "                               stride=stride)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=out_channels1, \n",
    "                               out_channels=out_channels2, \n",
    "                               kernel_size=kernel_size, \n",
    "                               stride=stride)\n",
    "        \n",
    "        # Determine the shape of self.conv2 output and pass it to linear1\n",
    "        dummy_input = torch.rand(1,*input_shape)\n",
    "        with torch.no_grad():\n",
    "            out_conv2_shape = torch.flatten(self.conv2(self.conv1(dummy_input))).shape[0]\n",
    "    \n",
    "        self.linear1 = nn.Linear(out_conv2_shape, hidden_size)\n",
    "        self.classifier = nn.Linear(hidden_size, num_actions)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of network\n",
    "        \"\"\"\n",
    "        # Convert to tensor if not type tensor\n",
    "        # and place it on cuda\n",
    "        if not x.is_cuda:\n",
    "            x = x.to('cuda')\n",
    "        \n",
    "        # Adjust tensor to have shape [batch, *image_shape]\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "This class is a final object which takes all previously constructed objects and orcestrates them into smooth training procedure. Additional to the previous parameters of *DQN Network* and *Environment*, we also need to specify here the parameters of **gamma**, i.e. the discounting factor used in computation of Bellman equation, **learning rate** used to update the weights of the network, **memory_update_samples** used to define the number of actions to take each training step to update the *Replay Memory*, **target_net_update_freq** and **memory_update_freq** used in defining the frequencies with respect to training steps to update *Replay Memory* and *Target Network*, and finally the parameter **tau** used as a soft update of target network as outlined in [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971). Note that the update rule in this implementation differs from the original since it stabilises the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(LightningModule):\n",
    "    \"\"\"\n",
    "    DQN Agent which handles all the training and interaction\n",
    "    between the network and environment\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 # Agent parameters\n",
    "                 gamma : float=0.99,\n",
    "                 learning_rate : float=1e-4,\n",
    "                 tau : float=1e-3,\n",
    "                 memory_update_samples : int=200,\n",
    "                 target_net_update_freq : int=2,\n",
    "                 memory_update_freq : int=2,\n",
    "                 # DQN Parameters\n",
    "                 input_shape : list=[4,96,96],\n",
    "                 out_channels1 : int=5,\n",
    "                 out_channels2 : int=5,\n",
    "                 kernel_size : int=4,\n",
    "                 stride : int=2,\n",
    "                 hidden_size : int=256,\n",
    "                 num_actions : int=5,\n",
    "                 # Environment Parameters\n",
    "                 batch_size : int=8,\n",
    "                 num_samples_memory : int=1000,\n",
    "                 observation_shape : list=[4, 96, 96],\n",
    "                 start_epsilon : float=0.0, \n",
    "                 epsilon_decrease_step : float=0.0, \n",
    "                 stop_epsilon : float=0.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Set Agent parameters\n",
    "        self.gamma = gamma\n",
    "        self.lr = learning_rate\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Set target network and base network\n",
    "        self.base_net = DQN(input_shape=input_shape,\n",
    "                            out_channels1=out_channels1,\n",
    "                            out_channels2=out_channels2,\n",
    "                            kernel_size=kernel_size,\n",
    "                            stride=stride,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_actions=num_actions)\n",
    "        \n",
    "        self.target_net = DQN(input_shape=input_shape,\n",
    "                            out_channels1=out_channels1,\n",
    "                            out_channels2=out_channels2,\n",
    "                            kernel_size=kernel_size,\n",
    "                            stride=stride,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_actions=num_actions)\n",
    "        \n",
    "        # Make target and base networks identical\n",
    "        self.target_net.load_state_dict(self.base_net.state_dict())\n",
    "        \n",
    "        # Create loss\n",
    "        self.loss = nn.MSELoss()\n",
    "        \n",
    "        # Create environemnt\n",
    "        self.env = Environment(batch_size=batch_size,\n",
    "                               num_samples_memory=num_samples_memory,\n",
    "                               observation_shape=observation_shape,\n",
    "                               start_epsilon=start_epsilon,\n",
    "                               epsilon_decrease_step=epsilon_decrease_step,\n",
    "                               stop_epsilon=stop_epsilon)\n",
    "        \n",
    "        # Create train step index\n",
    "        self.train_step_idx = 0\n",
    "        \n",
    "        # Set number of samples to update in training loop\n",
    "        self.memory_update_samples = memory_update_samples\n",
    "        \n",
    "        # Create parameters of memory and model update frequencies\n",
    "        self.target_net_update_freq = target_net_update_freq\n",
    "        self.memory_update_freq = memory_update_freq\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of base network\n",
    "        \"\"\"\n",
    "        x = self.base_net(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Run training step where an agent plays a game with \n",
    "        updating memory and updates network if permitted \n",
    "        by frequency\n",
    "        \"\"\"\n",
    "        \n",
    "        # RUN FORWARD AND BACKWARD PASSES\n",
    "        # ------------------------------------------------------\n",
    "        # Sample a batch and determine best action q-values\n",
    "        # for next_observation\n",
    "        current_observations, actions, rewards, end_states, next_observations = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            best_q_values = self.target_net(next_observations).max(dim=1).values\n",
    "            best_q_values[end_states] = 0\n",
    "            \n",
    "        \n",
    "        # Create y-tensor\n",
    "        y = rewards + self.gamma * best_q_values\n",
    "        y = y.reshape(-1,1)\n",
    "        \n",
    "        # Create q-values for current_observation\n",
    "        out = self.base_net(current_observations)\n",
    "        out = out[range(len(out)),actions.long()].reshape(-1,1)\n",
    "                \n",
    "        # Get loss function\n",
    "        loss = self.loss(y, out)\n",
    "        # ------------------------------------------------------\n",
    "        \n",
    "        # Update target network if needed\n",
    "        if self.train_step_idx % self.target_net_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Play a game and update replay memory if needed\n",
    "        if self.train_step_idx % self.memory_update_freq == 0:\n",
    "            self.env.play_game(n_steps=self.memory_update_samples, \n",
    "                               model=self.base_net,\n",
    "                               play_one_game=True,\n",
    "                               random_action=False)\n",
    "        \n",
    "        # Increment train_step idx\n",
    "        self.train_step_idx += 1\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update the weights of target network\n",
    "        \"\"\"\n",
    "        new_target_net_dict = self.target_net.state_dict().copy()\n",
    "        base_net_dict = self.base_net.state_dict().copy()\n",
    "        \n",
    "        for name in self.target_net.state_dict().copy().keys():\n",
    "            new_target_net_dict[name] = self.tau * base_net_dict[name] + \\\n",
    "                                            (1 - self.tau) * new_target_net_dict[name]\n",
    "            \n",
    "        self.target_net.load_state_dict(new_target_net_dict)\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configure optimiser for training\n",
    "        \"\"\"\n",
    "        # filter(lambda p: p.requires_grad, model.parameters()) allows optimizer to skip params of\n",
    "        # the pretrained model\n",
    "        optimizer = AdamW(filter(lambda p: p.requires_grad, self.parameters()), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Code\n",
    "\n",
    "The Next 3 cells outline the configuration and launch code to start DQN model. If you also want to experience the visualisations of training and logging, please input your *Wandb API KEY* into the corresponding field, end enjoy the results.\n",
    "\n",
    "I also reccomend running this code from the **Save Version** button -> **Save and Run All (Commit)** since it takes several hours to train a model.\n",
    "\n",
    "For my visualisations of the code, you can refer to my [report](https://wandb.ai/vladargunov/DQN%20Car%20Racing/reports/Summary-Report--VmlldzoyODQ1OTky?accessToken=2biech898sne4t9pwhqy54giuj04793n5jfo133rsb887piv5vla8jaa7pqvu9ro) where I store the successful (and not so successful) runs of my models together with the configuration that I used in each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Project Variables\n",
    "#########################\n",
    "\n",
    "USE_WANDB = False\n",
    "os.environ['WANDB_API_KEY'] = '0e1282269454018371df11ec32dacb75e1aa022a'\n",
    "PROJECT_NAME = 'DQN Car Racing'\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Agent parameters\n",
    "    gamma=0.95\n",
    "    learning_rate=1e-3\n",
    "    tau=0.05\n",
    "    memory_update_samples=10_000\n",
    "    target_net_update_freq=2\n",
    "    memory_update_freq=1\n",
    "    # DQN Parameters\n",
    "    input_shape=[4,96,96] # [channels, height, width] <- shape of observation to \n",
    "    # store in memory AFTER the preprocessing\n",
    "    out_channels1=16\n",
    "    out_channels2=32\n",
    "    kernel_size=4\n",
    "    stride=2 # Used for both conv layers\n",
    "    hidden_size=256 # Size of hidden linear layer after the 2nd convolution layer\n",
    "    num_actions=5\n",
    "    # Environment Parameters\n",
    "    batch_size=256\n",
    "    num_samples_memory=15_000\n",
    "    observation_shape=[4, 96, 96] # Must be equal to input_shape \n",
    "    start_epsilon=1\n",
    "    epsilon_decrease_step=1 / 30_000\n",
    "    stop_epsilon=0.01\n",
    "    # Trainer Parameters\n",
    "    num_train_steps = 6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raduc\\repos\\RAU_2023\\env\\lib\\site-packages\\pytorch_lightning\\loggers\\wandb.py:395: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Start filling Replay Memory...\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "# Train\n",
    "###############################\n",
    "\n",
    "def main():\n",
    "    \n",
    "    if not USE_WANDB:\n",
    "        os.environ['WANDB_MODE']= 'disabled'\n",
    "    \n",
    "    run = wandb.init(reinit=True, project=PROJECT_NAME)\n",
    "\n",
    "    # Enable wandb logger\n",
    "    wandb_logger = WandbLogger(log_model=True)\n",
    "\n",
    "    # Create agent\n",
    "    not_agent_cfg = ['num_train_steps']\n",
    "    agent_dict = {key : value for key, value in CFG.__dict__.items() if not key.startswith('__') \\\n",
    "               and key not in not_agent_cfg}\n",
    "    agent = Agent(**agent_dict)\n",
    "\n",
    "\n",
    "    # Log histograms of gradients and parameters and parameter histograms\n",
    "    wandb_logger.watch(agent, log='all')\n",
    "    \n",
    "    # Checkpoint callback to load the best model based on validation accuracy\n",
    "    checkpoint_callback = ModelCheckpoint(monitor=\"Game Total Return\", mode='max',save_top_k=1)\n",
    "\n",
    "    # Start trainer\n",
    "    trainer = Trainer(logger=wandb_logger,\n",
    "                      max_epochs=CFG.num_train_steps,\n",
    "                      reload_dataloaders_every_n_epochs=1,\n",
    "                      limit_train_batches=1,\n",
    "                      log_every_n_steps=1,\n",
    "                      accelerator='auto',\n",
    "                      fast_dev_run=DEBUG)\n",
    "\n",
    "    trainer.fit(model=agent, datamodule=agent.env)\n",
    "    \n",
    "\n",
    "    # Update configuration to the wandb\n",
    "    CFG_dict = {key : value for key, value in CFG.__dict__.items() if not key.startswith('__')}    \n",
    "    wandb_logger.experiment.config.update(CFG_dict)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing \n",
    "\n",
    "Finally, if you would like to test your model how it performs in real time, you can execute the next function to generate a folder with frames of a game and a corresponding action taken at each frame (named test_game.zip). You can downloaded it locally and see the result.\n",
    "\n",
    "I uploaded a [dataset](https://www.kaggle.com/datasets/vladargunov/gym-car-racing-dqn-model) which contains a model of one of the successful runs, so if you would like to try it out, you can run the next cell, also initially setting **TEST_STAGE = True**. No need to train a model beforehand, so do not execute the previous cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent():\n",
    "    # Set download path\n",
    "    #model_dir = '../input/gym-car-racing-dqn-model/model.ckpt'\n",
    "    #model_dir = 'C:/Users/raduc/repos/RAU_2023/input/gym-car-racing-dqn-model/model.ckpt'\n",
    "    model_dir = \"epoch=299-step=300.ckpt\"\n",
    "    # Initialise test model\n",
    "    not_agent_cfg = ['num_train_steps']\n",
    "    agent_dict = {key : value for key, value in CFG.__dict__.items() if not key.startswith('__') \\\n",
    "                   and key not in not_agent_cfg}\n",
    "\n",
    "    agent_dict['num_samples_memory'] = 5 # Set it to minimal value since we do not need memory replay here\n",
    "    \n",
    "    test_agent = Agent(**agent_dict)\n",
    "    test_model = test_agent.target_net\n",
    "    \n",
    "    test_model.load_state_dict(torch.load(model_dir), strict=False)\n",
    "    test_model.to('cuda')\n",
    "    # Setup environment\n",
    "    test_agent.env.setup()\n",
    "    \n",
    "    # Test model\n",
    "    test_agent.env.play_game(model=test_model, predict_stage=True, play_one_game=True)\n",
    "    # Remove previous zip folder if it exists\n",
    "    #!rm test_game.zip\n",
    "    # Zip folder with frames\n",
    "    #!zip -r test_game.zip ./test_game -q\n",
    "    # Remove original folder\n",
    "    #!rm -r test_game\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Start filling Replay Memory...\n",
      "...Replay memory is filled...\n"
     ]
    }
   ],
   "source": [
    "TEST_STAGE = True\n",
    "\n",
    "\n",
    "if TEST_STAGE:\n",
    "    test_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: After a couple of test runs I discovered that a car many times opts for braking before turns so it does not lose points. I think in the next notebook we can adress this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Well, that's it, in the notebook we have managed to run the model so it gives positive score, but since we have worked in discrete environment we could not achieve the best outcome. Yet it is a great start!\n",
    "\n",
    "I hope you liked this notebook - if you find any mistakes here or would like to leave a comment, you can do it here or you can also mail me if you hesitate to say it in public.\n",
    "\n",
    "My mail: argunovvlad5@gmail.com\n",
    "\n",
    "Hope to do better next time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
